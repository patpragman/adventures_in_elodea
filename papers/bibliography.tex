@article{1910.13796v1,
Author        = {Niall O' Mahony and Sean Campbell and Anderson Carvalho and Suman Harapanahalli and Gustavo Velasco-Hernandez and Lenka Krpalkova and Daniel Riordan and Joseph Walsh},
Title         = {Deep Learning vs. Traditional Computer Vision},
Eprint        = {1910.13796v1},
DOI           = {10.1007/978-3-030-17795-9},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Deep Learning has pushed the limits of what was possible in the domain of
Digital Image Processing. However, that is not to say that the traditional
computer vision techniques which had been undergoing progressive development in
years prior to the rise of DL have become obsolete. This paper will analyse the
benefits and drawbacks of each approach. The aim of this paper is to promote a
discussion on whether knowledge of classical computer vision techniques should
be maintained. The paper will also explore how the two sides of computer vision
can be combined. Several recent hybrid methodologies are reviewed which have
demonstrated the ability to improve computer vision performance and to tackle
problems not suited to Deep Learning. For example, combining traditional
computer vision techniques with Deep Learning has been popular in emerging
domains such as Panoramic Vision and 3D vision for which Deep Learning models
have not yet been fully optimised},
Year          = {2019},
Month         = {Oct},
Note          = {in Advances in Computer Vision Proceedings of the 2019 Computer
  Vision Conference (CVC). Springer Nature Switzerland AG, pp. 128-144},
Url           = {http://arxiv.org/abs/1910.13796v1},
File          = {1910.13796v1.pdf}
}
@article{1510.05275v1,
Author        = {Hongmin Li and Pei Jing and Guoqi Li},
Title         = {Real-time Tracking Based on Neuromrophic Vision},
Eprint        = {1510.05275v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Real-time tracking is an important problem in computer vision in which most
methods are based on the conventional cameras. Neuromorphic vision is a concept
defined by incorporating neuromorphic vision sensors such as silicon retinas in
vision processing system. With the development of the silicon technology,
asynchronous event-based silicon retinas that mimic neuro-biological
architectures has been developed in recent years. In this work, we combine the
vision tracking algorithm of computer vision with the information encoding
mechanism of event-based sensors which is inspired from the neural rate coding
mechanism. The real-time tracking of single object with the advantage of high
speed of 100 time bins per second is successfully realized. Our method
demonstrates that the computer vision methods could be used for the
neuromorphic vision processing and we can realize fast real-time tracking using
neuromorphic vision sensors compare to the conventional camera.},
Year          = {2015},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1510.05275v1},
File          = {1510.05275v1.pdf}
}
@article{1705.04352v3,
Author        = {Mark Buckler and Suren Jayasuriya and Adrian Sampson},
Title         = {Reconfiguring the Imaging Pipeline for Computer Vision},
Eprint        = {1705.04352v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Advancements in deep learning have ignited an explosion of research on
efficient hardware for embedded computer vision. Hardware vision acceleration,
however, does not address the cost of capturing and processing the image data
that feeds these algorithms. We examine the role of the image signal processing
(ISP) pipeline in computer vision to identify opportunities to reduce
computation and save energy. The key insight is that imaging pipelines should
be designed to be configurable: to switch between a traditional photography
mode and a low-power vision mode that produces lower-quality image data
suitable only for computer vision. We use eight computer vision algorithms and
a reversible pipeline simulation tool to study the imaging system's impact on
vision performance. For both CNN-based and classical vision algorithms, we
observe that only two ISP stages, demosaicing and gamma compression, are
critical for task performance. We propose a new image sensor design that can
compensate for skipping these stages. The sensor design features an adjustable
resolution and tunable analog-to-digital converters (ADCs). Our proposed
imaging system's vision mode disables the ISP entirely and configures the
sensor to produce subsampled, lower-precision image data. This vision mode can
save ~75% of the average energy of a baseline photography mode while having
only a small impact on vision task accuracy.},
Year          = {2017},
Month         = {May},
Url           = {http://arxiv.org/abs/1705.04352v3},
File          = {1705.04352v3.pdf}
}
@article{2301.05065v1,
Author        = {Xinsong Zhang and Yan Zeng and Jipeng Zhang and Hang Li},
Title         = {Toward Building General Foundation Models for Language, Vision, and
  Vision-Language Understanding Tasks},
Eprint        = {2301.05065v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Foundation models or pre-trained models have substantially improved the
performance of various language, vision, and vision-language understanding
tasks. However, existing foundation models can only perform the best in one
type of tasks, namely language, vision, or vision-language. It is still an open
question whether it is possible to construct a foundation model performing the
best for all the understanding tasks, which we call a general foundation model.
In this paper, we propose a new general foundation model, X-FM (the
X-Foundation Model). X-FM has one language encoder, one vision encoder, and one
fusion encoder, as well as a new training method. The training method includes
two new techniques for learning X-FM from text, image, and image-text pair
data. One is to stop gradients from the vision-language training when learning
the language encoder. The other is to leverage the vision-language training to
guide the learning of the vision encoder. Extensive experiments on benchmark
datasets show that X-FM can significantly outperform existing general
foundation models and perform better than or comparable to existing foundation
models specifically for language, vision, or vision-language understanding.},
Year          = {2023},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2301.05065v1},
File          = {2301.05065v1.pdf}
}
@article{2004.02810v1,
Author        = {Jasmin Hundall and Benson A. Babu},
Title         = {Computer Vision and Abnormal Patient Gait Assessment a Comparison of
  Machine Learning Models},
Eprint        = {2004.02810v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Abnormal gait, its associated falls and complications have high patient
morbidity, mortality. Computer vision detects, predicts patient gait
abnormalities, assesses fall risk and serves as clinical decision support tool
for physicians. This paper performs a systematic review of how computer vision,
machine learning models perform an abnormal patient's gait assessment. Computer
vision is beneficial in gait analysis, it helps capture the patient posture.
Several literature suggests the use of different machine learning algorithms
such as SVM, ANN, K-Star, Random Forest, KNN, among others to perform the
classification on the features extracted to study patient gait abnormalities.},
Year          = {2020},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2004.02810v1},
File          = {2004.02810v1.pdf}
}
@article{2302.08242v1,
Author        = {André Susano Pinto and Alexander Kolesnikov and Yuge Shi and Lucas Beyer and Xiaohua Zhai},
Title         = {Tuning computer vision models with task rewards},
Eprint        = {2302.08242v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Misalignment between model predictions and intended usage can be detrimental
for the deployment of computer vision models. The issue is exacerbated when the
task involves complex structured outputs, as it becomes harder to design
procedures which address this misalignment. In natural language processing,
this is often addressed using reinforcement learning techniques that align
models with a task reward. We adopt this approach and show its surprising
effectiveness across multiple computer vision tasks, such as object detection,
panoptic segmentation, colorization and image captioning. We believe this
approach has the potential to be widely useful for better aligning models with
a diverse range of computer vision tasks.},
Year          = {2023},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2302.08242v1},
File          = {2302.08242v1.pdf}
}
@article{1910.02989v2,
Author        = {Kai Zhang and Jin Sun and Noah Snavely},
Title         = {Leveraging Vision Reconstruction Pipelines for Satellite Imagery},
Eprint        = {1910.02989v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Reconstructing 3D geometry from satellite imagery is an important topic of
research. However, disparities exist between how this 3D reconstruction problem
is handled in the remote sensing context and how multi-view reconstruction
pipelines have been developed in the computer vision community. In this paper,
we explore whether state-of-the-art reconstruction pipelines from the vision
community can be applied to the satellite imagery. Along the way, we address
several challenges adapting vision-based structure from motion and multi-view
stereo methods. We show that vision pipelines can offer competitive speed and
accuracy in the satellite context.},
Year          = {2019},
Month         = {Oct},
Url           = {http://arxiv.org/abs/1910.02989v2},
File          = {1910.02989v2.pdf}
}
@article{2102.00297v1,
Author        = {Nicole Han and Sudhanshu Srivastava and Aiwen Xu and Devi Klein and Michael Beyeler},
Title         = {Deep Learning--Based Scene Simplification for Bionic Vision},
Eprint        = {2102.00297v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Retinal degenerative diseases cause profound visual impairment in more than
10 million people worldwide, and retinal prostheses are being developed to
restore vision to these individuals. Analogous to cochlear implants, these
devices electrically stimulate surviving retinal cells to evoke visual percepts
(phosphenes). However, the quality of current prosthetic vision is still
rudimentary. Rather than aiming to restore "natural" vision, there is potential
merit in borrowing state-of-the-art computer vision algorithms as image
processing techniques to maximize the usefulness of prosthetic vision. Here we
combine deep learning--based scene simplification strategies with a
psychophysically validated computational model of the retina to generate
realistic predictions of simulated prosthetic vision, and measure their ability
to support scene understanding of sighted subjects (virtual patients) in a
variety of outdoor scenarios. We show that object segmentation may better
support scene understanding than models based on visual saliency and monocular
depth estimation. In addition, we highlight the importance of basing
theoretical predictions on biologically realistic models of phosphene shape.
Overall, this work has the potential to drastically improve the utility of
prosthetic vision for people blinded from retinal degenerative diseases.},
Year          = {2021},
Month         = {Jan},
Url           = {http://arxiv.org/abs/2102.00297v1},
File          = {2102.00297v1.pdf}
}
@article{2306.05135v1,
Author        = {Håkon Hukkelås and Frank Lindseth},
Title         = {Does Image Anonymization Impact Computer Vision Training?},
Eprint        = {2306.05135v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Image anonymization is widely adapted in practice to comply with privacy
regulations in many regions. However, anonymization often degrades the quality
of the data, reducing its utility for computer vision development. In this
paper, we investigate the impact of image anonymization for training computer
vision models on key computer vision tasks (detection, instance segmentation,
and pose estimation). Specifically, we benchmark the recognition drop on common
detection datasets, where we evaluate both traditional and realistic
anonymization for faces and full bodies. Our comprehensive experiments reflect
that traditional image anonymization substantially impacts final model
performance, particularly when anonymizing the full body. Furthermore, we find
that realistic anonymization can mitigate this decrease in performance, where
our experiments reflect a minimal performance drop for face anonymization. Our
study demonstrates that realistic anonymization can enable privacy-preserving
computer vision development with minimal performance degradation across a range
of important computer vision benchmarks.},
Year          = {2023},
Month         = {Jun},
Url           = {http://arxiv.org/abs/2306.05135v1},
File          = {2306.05135v1.pdf}
}
@article{1803.11232v1,
Author        = {Yuhao Zhu and Anand Samajdar and Matthew Mattina and Paul Whatmough},
Title         = {Euphrates: Algorithm-SoC Co-Design for Low-Power Mobile Continuous
  Vision},
Eprint        = {1803.11232v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Continuous computer vision (CV) tasks increasingly rely on convolutional
neural networks (CNN). However, CNNs have massive compute demands that far
exceed the performance and energy constraints of mobile devices. In this paper,
we propose and develop an algorithm-architecture co-designed system, Euphrates,
that simultaneously improves the energy-efficiency and performance of
continuous vision tasks.
  Our key observation is that changes in pixel data between consecutive frames
represents visual motion. We first propose an algorithm that leverages this
motion information to relax the number of expensive CNN inferences required by
continuous vision applications. We co-design a mobile System-on-a-Chip (SoC)
architecture to maximize the efficiency of the new algorithm. The key to our
architectural augmentation is to co-optimize different SoC IP blocks in the
vision pipeline collectively. Specifically, we propose to expose the motion
data that is naturally generated by the Image Signal Processor (ISP) early in
the vision pipeline to the CNN engine. Measurement and synthesis results show
that Euphrates achieves up to 66% SoC-level energy savings (4 times for the
vision computations), with only 1% accuracy loss.},
Year          = {2018},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1803.11232v1},
File          = {1803.11232v1.pdf}
}
@article{2103.15358v2,
Author        = {Pengchuan Zhang and Xiyang Dai and Jianwei Yang and Bin Xiao and Lu Yuan and Lei Zhang and Jianfeng Gao},
Title         = {Multi-Scale Vision Longformer: A New Vision Transformer for
  High-Resolution Image Encoding},
Eprint        = {2103.15358v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {This paper presents a new Vision Transformer (ViT) architecture Multi-Scale
Vision Longformer, which significantly enhances the ViT of
\cite{dosovitskiy2020image} for encoding high-resolution images using two
techniques. The first is the multi-scale model structure, which provides image
encodings at multiple scales with manageable computational cost. The second is
the attention mechanism of vision Longformer, which is a variant of Longformer
\cite{beltagy2020longformer}, originally developed for natural language
processing, and achieves a linear complexity w.r.t. the number of input tokens.
A comprehensive empirical study shows that the new ViT significantly
outperforms several strong baselines, including the existing ViT models and
their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent
work \cite{wang2021pyramid}, on a range of vision tasks, including image
classification, object detection, and segmentation. The models and source code
are released at \url{https://github.com/microsoft/vision-longformer}.},
Year          = {2021},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2103.15358v2},
File          = {2103.15358v2.pdf}
}
@article{1802.06464v3,
Author        = {Tat-Jun Chin and Zhipeng Cai and Frank Neumann},
Title         = {Robust Fitting in Computer Vision: Easy or Hard?},
Eprint        = {1802.06464v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Robust model fitting plays a vital role in computer vision, and research into
algorithms for robust fitting continues to be active. Arguably the most popular
paradigm for robust fitting in computer vision is consensus maximisation, which
strives to find the model parameters that maximise the number of inliers.
Despite the significant developments in algorithms for consensus maximisation,
there has been a lack of fundamental analysis of the problem in the computer
vision literature. In particular, whether consensus maximisation is "tractable"
remains a question that has not been rigorously dealt with, thus making it
difficult to assess and compare the performance of proposed algorithms,
relative to what is theoretically achievable. To shed light on these issues, we
present several computational hardness results for consensus maximisation. Our
results underline the fundamental intractability of the problem, and resolve
several ambiguities existing in the literature.},
Year          = {2018},
Month         = {Feb},
Note          = {European Conference on Computer Vision (ECCV), 2018},
Url           = {http://arxiv.org/abs/1802.06464v3},
File          = {1802.06464v3.pdf}
}
@article{2012.12235v1,
Author        = {Hadi Salman and Andrew Ilyas and Logan Engstrom and Sai Vemprala and Aleksander Madry and Ashish Kapoor},
Title         = {Unadversarial Examples: Designing Objects for Robust Vision},
Eprint        = {2012.12235v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {We study a class of realistic computer vision settings wherein one can
influence the design of the objects being recognized. We develop a framework
that leverages this capability to significantly improve vision models'
performance and robustness. This framework exploits the sensitivity of modern
machine learning algorithms to input perturbations in order to design "robust
objects," i.e., objects that are explicitly optimized to be confidently
detected or classified. We demonstrate the efficacy of the framework on a wide
variety of vision-based tasks ranging from standard benchmarks, to
(in-simulation) robotics, to real-world experiments. Our code can be found at
https://git.io/unadversarial .},
Year          = {2020},
Month         = {Dec},
Url           = {http://arxiv.org/abs/2012.12235v1},
File          = {2012.12235v1.pdf}
}
@article{1912.04217v1,
Author        = {Tom White},
Title         = {Shared Visual Abstractions},
Eprint        = {1912.04217v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {This paper presents abstract art created by neural networks and broadly
recognizable across various computer vision systems. The existence of abstract
forms that trigger specific labels independent of neural architecture or
training set suggests convolutional neural networks build shared visual
representations for the categories they understand. Computer vision classifiers
encountering these drawings often respond with strong responses for specific
labels - in extreme cases stronger than all examples from the validation set.
By surveying human subjects we confirm that these abstract artworks are also
broadly recognizable by people, suggesting visual representations triggered by
these drawings are shared across human and computer vision systems.},
Year          = {2019},
Month         = {Nov},
Url           = {http://arxiv.org/abs/1912.04217v1},
File          = {1912.04217v1.pdf}
}
@article{2111.11066v1,
Author        = {Chaoyang He and Alay Dilipbhai Shah and Zhenheng Tang and Di Fan1Adarshan Naiynar Sivashunmugam and Keerti Bhogaraju and Mita Shimpi and Li Shen and Xiaowen Chu and Mahdi Soltanolkotabi and Salman Avestimehr},
Title         = {FedCV: A Federated Learning Framework for Diverse Computer Vision Tasks},
Eprint        = {2111.11066v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Federated Learning (FL) is a distributed learning paradigm that can learn a
global or personalized model from decentralized datasets on edge devices.
However, in the computer vision domain, model performance in FL is far behind
centralized training due to the lack of exploration in diverse tasks with a
unified FL framework. FL has rarely been demonstrated effectively in advanced
computer vision tasks such as object detection and image segmentation. To
bridge the gap and facilitate the development of FL for computer vision tasks,
in this work, we propose a federated learning library and benchmarking
framework, named FedCV, to evaluate FL on the three most representative
computer vision tasks: image classification, image segmentation, and object
detection. We provide non-I.I.D. benchmarking datasets, models, and various
reference FL algorithms. Our benchmark study suggests that there are multiple
challenges that deserve future exploration: centralized training tricks may not
be directly applied to FL; the non-I.I.D. dataset actually downgrades the model
accuracy to some degree in different tasks; improving the system efficiency of
federated training is challenging given the huge number of parameters and the
per-client memory cost. We believe that such a library and benchmark, along
with comparable evaluation settings, is necessary to make meaningful progress
in FL on computer vision tasks. FedCV is publicly available:
https://github.com/FedML-AI/FedCV.},
Year          = {2021},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2111.11066v1},
File          = {2111.11066v1.pdf}
}
@article{0909.1608v1,
Author        = {G. Chen and G. Lerman},
Title         = {Motion Segmentation by SCC on the Hopkins 155 Database},
Eprint        = {0909.1608v1},
DOI           = {10.1109/ICCVW.2009.5457626},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {We apply the Spectral Curvature Clustering (SCC) algorithm to a benchmark
database of 155 motion sequences, and show that it outperforms all other
state-of-the-art methods. The average misclassification rate by SCC is 1.41%
for sequences having two motions and 4.85% for three motions.},
Year          = {2009},
Month         = {Sep},
Note          = {Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th
  International Conference on, 2009, pp. 759 - 764},
Url           = {http://arxiv.org/abs/0909.1608v1},
File          = {0909.1608v1.pdf}
}
@article{1701.06859v1,
Author        = {Laurent Perrinet},
Title         = {Sparse models for Computer Vision},
Eprint        = {1701.06859v1},
DOI           = {10.1002/9783527680863.ch14},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {The representation of images in the brain is known to be sparse. That is, as
neural activity is recorded in a visual area ---for instance the primary visual
cortex of primates--- only a few neurons are active at a given time with
respect to the whole population. It is believed that such a property reflects
the efficient match of the representation with the statistics of natural
scenes. Applying such a paradigm to computer vision therefore seems a promising
approach towards more biomimetic algorithms. Herein, we will describe a
biologically-inspired approach to this problem. First, we will describe an
unsupervised learning paradigm which is particularly adapted to the efficient
coding of image patches. Then, we will outline a complete multi-scale framework
---SparseLets--- implementing a biologically inspired sparse representation of
natural images. Finally, we will propose novel methods for integrating prior
information into these algorithms and provide some preliminary experimental
results. We will conclude by giving some perspective on applying such
algorithms to computer vision. More specifically, we will propose that
bio-inspired approaches may be applied to computer vision using predictive
coding schemes, sparse models being one simple and efficient instance of such
schemes.},
Year          = {2017},
Month         = {Jan},
Note          = {Biologically inspired computer vision, 2015, 9783527680863},
Url           = {http://arxiv.org/abs/1701.06859v1},
File          = {1701.06859v1.pdf}
}
@article{1906.09677v1,
Author        = {Lucas Jaffe and Michael Zelinski and Wesam Sakla},
Title         = {Remote Sensor Design for Visual Recognition with Convolutional Neural
  Networks},
Eprint        = {1906.09677v1},
DOI           = {10.1109/TGRS.2019.2925813},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.IV},
Abstract      = {While deep learning technologies for computer vision have developed rapidly
since 2012, modeling of remote sensing systems has remained focused around
human vision. In particular, remote sensing systems are usually constructed to
optimize sensing cost-quality trade-offs with respect to human image
interpretability. While some recent studies have explored remote sensing system
design as a function of simple computer vision algorithm performance, there has
been little work relating this design to the state-of-the-art in computer
vision: deep learning with convolutional neural networks. We develop
experimental systems to conduct this analysis, showing results with modern deep
learning algorithms and recent overhead image data. Our results are compared to
standard image quality measurements based on human visual perception, and we
conclude not only that machine and human interpretability differ significantly,
but that computer vision performance is largely self-consistent across a range
of disparate conditions. This research is presented as a cornerstone for a new
generation of sensor design systems which focus on computer algorithm
performance instead of human visual perception.},
Year          = {2019},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1906.09677v1},
File          = {1906.09677v1.pdf}
}
@article{1905.12887v2,
Author        = {Brady Zhou and Philipp Krähenbühl and Vladlen Koltun},
Title         = {Does computer vision matter for action?},
Eprint        = {1905.12887v2},
DOI           = {10.1126/scirobotics.aaw6661},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Computer vision produces representations of scene content. Much computer
vision research is predicated on the assumption that these intermediate
representations are useful for action. Recent work at the intersection of
machine learning and robotics calls this assumption into question by training
sensorimotor systems directly for the task at hand, from pixels to actions,
with no explicit intermediate representations. Thus the central question of our
work: Does computer vision matter for action? We probe this question and its
offshoots via immersive simulation, which allows us to conduct controlled
reproducible experiments at scale. We instrument immersive three-dimensional
environments to simulate challenges such as urban driving, off-road trail
traversal, and battle. Our main finding is that computer vision does matter.
Models equipped with intermediate representations train faster, achieve higher
task performance, and generalize better to previously unseen environments. A
video that summarizes the work and illustrates the results can be found at
https://youtu.be/4MfWa2yZ0Jc},
Year          = {2019},
Month         = {May},
Note          = {Science Robotics 22 May 2019: Vol. 4, Issue 30, eaaw6661},
Url           = {http://arxiv.org/abs/1905.12887v2},
File          = {1905.12887v2.pdf}
}
@article{1912.07726v1,
Author        = {Kaiyu Yang and Klint Qinami and Li Fei-Fei and Jia Deng and Olga Russakovsky},
Title         = {Towards Fairer Datasets: Filtering and Balancing the Distribution of the
  People Subtree in the ImageNet Hierarchy},
Eprint        = {1912.07726v1},
DOI           = {10.1145/3351095.3375709},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Computer vision technology is being used by many but remains representative
of only a few. People have reported misbehavior of computer vision models,
including offensive prediction results and lower performance for
underrepresented groups. Current computer vision models are typically developed
using datasets consisting of manually annotated images or videos; the data and
label distributions in these datasets are critical to the models' behavior. In
this paper, we examine ImageNet, a large-scale ontology of images that has
spurred the development of many modern computer vision methods. We consider
three key factors within the "person" subtree of ImageNet that may lead to
problematic behavior in downstream computer vision technology: (1) the stagnant
concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of
all categories with images, and (3) the inequality of representation in the
images within concepts. We seek to illuminate the root causes of these concerns
and take the first steps to mitigate them constructively.},
Year          = {2019},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1912.07726v1},
File          = {1912.07726v1.pdf}
}
@article{2108.11510v1,
Author        = {Ngan Le and Vidhiwar Singh Rathour and Kashu Yamazaki and Khoa Luu and Marios Savvides},
Title         = {Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey},
Eprint        = {2108.11510v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Deep reinforcement learning augments the reinforcement learning framework and
utilizes the powerful representation of deep neural networks. Recent works have
demonstrated the remarkable successes of deep reinforcement learning in various
domains including finance, medicine, healthcare, video games, robotics, and
computer vision. In this work, we provide a detailed review of recent and
state-of-the-art research advances of deep reinforcement learning in computer
vision. We start with comprehending the theories of deep learning,
reinforcement learning, and deep reinforcement learning. We then propose a
categorization of deep reinforcement learning methodologies and discuss their
advantages and limitations. In particular, we divide deep reinforcement
learning into seven main categories according to their applications in computer
vision, i.e. (i)landmark localization (ii) object detection; (iii) object
tracking; (iv) registration on both 2D image and 3D image volumetric data (v)
image segmentation; (vi) videos analysis; and (vii) other applications. Each of
these categories is further analyzed with reinforcement learning techniques,
network design, and performance. Moreover, we provide a comprehensive analysis
of the existing publicly available datasets and examine source code
availability. Finally, we present some open issues and discuss future research
directions on deep reinforcement learning in computer vision},
Year          = {2021},
Month         = {Aug},
Url           = {http://arxiv.org/abs/2108.11510v1},
File          = {2108.11510v1.pdf}
}
@article{2207.10741v1,
Author        = {Caleb Tung and Abhinav Goel and Xiao Hu and Nicholas Eliopoulos and Emmanuel Amobi and George K. Thiruvathukal and Vipin Chaudhary and Yung-Hsiang Lu},
Title         = {Irrelevant Pixels are Everywhere: Find and Exclude Them for More
  Efficient Computer Vision},
Eprint        = {2207.10741v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Computer vision is often performed using Convolutional Neural Networks
(CNNs). CNNs are compute-intensive and challenging to deploy on
power-contrained systems such as mobile and Internet-of-Things (IoT) devices.
CNNs are compute-intensive because they indiscriminately compute many features
on all pixels of the input image. We observe that, given a computer vision
task, images often contain pixels that are irrelevant to the task. For example,
if the task is looking for cars, pixels in the sky are not very useful.
Therefore, we propose that a CNN be modified to only operate on relevant pixels
to save computation and energy. We propose a method to study three popular
computer vision datasets, finding that 48% of pixels are irrelevant. We also
propose the focused convolution to modify a CNN's convolutional layers to
reject the pixels that are marked irrelevant. On an embedded device, we observe
no loss in accuracy, while inference latency, energy consumption, and
multiply-add count are all reduced by about 45%.},
Year          = {2022},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2207.10741v1},
File          = {2207.10741v1.pdf}
}
@article{1906.11879v1,
Author        = {Murad Qasaimeh and Kristof Denolf and Jack Lo and Kees Vissers and Joseph Zambreno and Phillip H. Jones},
Title         = {Comparing Energy Efficiency of CPU, GPU and FPGA Implementations for
  Vision Kernels},
Eprint        = {1906.11879v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Developing high performance embedded vision applications requires balancing
run-time performance with energy constraints. Given the mix of hardware
accelerators that exist for embedded computer vision (e.g. multi-core CPUs,
GPUs, and FPGAs), and their associated vendor optimized vision libraries, it
becomes a challenge for developers to navigate this fragmented solution space.
To aid with determining which embedded platform is most suitable for their
application, we conduct a comprehensive benchmark of the run-time performance
and energy efficiency of a wide range of vision kernels. We discuss rationales
for why a given underlying hardware architecture innately performs well or
poorly based on the characteristics of a range of vision kernel categories.
Specifically, our study is performed for three commonly used HW accelerators
for embedded vision applications: ARM57 CPU, Jetson TX2 GPU and ZCU102 FPGA,
using their vendor optimized vision libraries: OpenCV, VisionWorks and
xfOpenCV. Our results show that the GPU achieves an energy/frame reduction
ratio of 1.1-3.2x compared to the others for simple kernels. While for more
complicated kernels and complete vision pipelines, the FPGA outperforms the
others with energy/frame reduction ratios of 1.2-22.3x. It is also observed
that the FPGA performs increasingly better as a vision application's pipeline
complexity grows.},
Year          = {2019},
Month         = {May},
Url           = {http://arxiv.org/abs/1906.11879v1},
File          = {1906.11879v1.pdf}
}
@article{2111.12293v2,
Author        = {Zhihang Yuan and Chenhao Xue and Yiqi Chen and Qiang Wu and Guangyu Sun},
Title         = {PTQ4ViT: Post-Training Quantization Framework for Vision Transformers
  with Twin Uniform Quantization},
Eprint        = {2111.12293v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Quantization is one of the most effective methods to compress neural
networks, which has achieved great success on convolutional neural networks
(CNNs). Recently, vision transformers have demonstrated great potential in
computer vision. However, previous post-training quantization methods performed
not well on vision transformer, resulting in more than 1% accuracy drop even in
8-bit quantization. Therefore, we analyze the problems of quantization on
vision transformers. We observe the distributions of activation values after
softmax and GELU functions are quite different from the Gaussian distribution.
We also observe that common quantization metrics, such as MSE and cosine
distance, are inaccurate to determine the optimal scaling factor. In this
paper, we propose the twin uniform quantization method to reduce the
quantization error on these activation values. And we propose to use a Hessian
guided metric to evaluate different scaling factors, which improves the
accuracy of calibration at a small cost. To enable the fast quantization of
vision transformers, we develop an efficient framework, PTQ4ViT. Experiments
show the quantized vision transformers achieve near-lossless prediction
accuracy (less than 0.5% drop at 8-bit quantization) on the ImageNet
classification task.},
Year          = {2021},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2111.12293v2},
File          = {2111.12293v2.pdf}
}
@article{2111.14725v1,
Author        = {Minghao Chen and Kan Wu and Bolin Ni and Houwen Peng and Bei Liu and Jianlong Fu and Hongyang Chao and Haibin Ling},
Title         = {Searching the Search Space of Vision Transformer},
Eprint        = {2111.14725v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Vision Transformer has shown great visual representation power in substantial
vision tasks such as recognition and detection, and thus been attracting
fast-growing efforts on manually designing more effective architectures. In
this paper, we propose to use neural architecture search to automate this
process, by searching not only the architecture but also the search space. The
central idea is to gradually evolve different search dimensions guided by their
E-T Error computed using a weight-sharing supernet. Moreover, we provide design
guidelines of general vision transformers with extensive analysis according to
the space searching process, which could promote the understanding of vision
transformer. Remarkably, the searched models, named S3 (short for Searching the
Search Space), from the searched space achieve superior performance to recently
proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The
effectiveness of S3 is also illustrated on object detection, semantic
segmentation and visual question answering, demonstrating its generality to
downstream vision and vision-language tasks. Code and models will be available
at https://github.com/microsoft/Cream.},
Year          = {2021},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2111.14725v1},
File          = {2111.14725v1.pdf}
}
@article{1903.08131v1,
Author        = {Corinne Jones and Vincent Roulet and Zaid Harchaoui},
Title         = {Kernel-based Translations of Convolutional Networks},
Eprint        = {1903.08131v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {Convolutional Neural Networks, as most artificial neural networks, are
commonly viewed as methods different in essence from kernel-based methods. We
provide a systematic translation of Convolutional Neural Networks (ConvNets)
into their kernel-based counterparts, Convolutional Kernel Networks (CKNs), and
demonstrate that this perception is unfounded both formally and empirically. We
show that, given a Convolutional Neural Network, we can design a corresponding
Convolutional Kernel Network, easily trainable using a new stochastic gradient
algorithm based on an accurate gradient computation, that performs on par with
its Convolutional Neural Network counterpart. We present experimental results
supporting our claims on landmark ConvNet architectures comparing each ConvNet
to its CKN counterpart over several parameter settings.},
Year          = {2019},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1903.08131v1},
File          = {1903.08131v1.pdf}
}
@article{2211.09983v1,
Author        = {Geonho Hwang and Myungjoo Kang},
Title         = {Universal Property of Convolutional Neural Networks},
Eprint        = {2211.09983v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NE},
Abstract      = {Universal approximation, whether a set of functions can approximate an
arbitrary function in a specific function space, has been actively studied in
recent years owing to the significant development of neural networks. However,
despite its extensive use, research on the universal properties of the
convolutional neural network has been limited due to its complex nature. In
this regard, we demonstrate the universal approximation theorem for
convolutional neural networks. A convolution with padding outputs the data of
the same shape as the input data; therefore, it is necessary to prove whether a
convolutional neural network composed of convolutions can approximate such a
function. We have shown that convolutional neural networks can approximate
continuous functions whose input and output values have the same shape. In
addition, the minimum depth of the neural network required for approximation
was presented, and we proved that it is the optimal value. We also verified
that convolutional neural networks with sufficiently deep layers have
universality when the number of channels is limited.},
Year          = {2022},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2211.09983v1},
File          = {2211.09983v1.pdf}
}
@article{1609.01000v1,
Author        = {Yuchen Zhang and Percy Liang and Martin J. Wainwright},
Title         = {Convexified Convolutional Neural Networks},
Eprint        = {1609.01000v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We describe the class of convexified convolutional neural networks (CCNNs),
which capture the parameter sharing of convolutional neural networks in a
convex manner. By representing the nonlinear convolutional filters as vectors
in a reproducing kernel Hilbert space, the CNN parameters can be represented as
a low-rank matrix, which can be relaxed to obtain a convex optimization
problem. For learning two-layer convolutional neural networks, we prove that
the generalization error obtained by a convexified CNN converges to that of the
best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise
manner. Empirically, CCNNs achieve performance competitive with CNNs trained by
backpropagation, SVMs, fully-connected neural networks, stacked denoising
auto-encoders, and other baseline methods.},
Year          = {2016},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1609.01000v1},
File          = {1609.01000v1.pdf}
}
@article{1804.05712v1,
Author        = {Hans Pinckaers and Geert Litjens},
Title         = {Training convolutional neural networks with megapixel images},
Eprint        = {1804.05712v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {To train deep convolutional neural networks, the input data and the
intermediate activations need to be kept in memory to calculate the gradient
descent step. Given the limited memory available in the current generation
accelerator cards, this limits the maximum dimensions of the input data. We
demonstrate a method to train convolutional neural networks holding only parts
of the image in memory while giving equivalent results. We quantitatively
compare this new way of training convolutional neural networks with
conventional training. In addition, as a proof of concept, we train a
convolutional neural network with 64 megapixel images, which requires 97% less
memory than the conventional approach.},
Year          = {2018},
Month         = {Apr},
Url           = {http://arxiv.org/abs/1804.05712v1},
File          = {1804.05712v1.pdf}
}
@article{2105.10559v2,
Author        = {Tianyu Ma and Adrian V. Dalca and Mert R. Sabuncu},
Title         = {Hyper-Convolution Networks for Biomedical Image Segmentation},
Eprint        = {2105.10559v2},
DOI           = {10.1109/WACV51458.2022.00205},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.IV},
Abstract      = {The convolution operation is a central building block of neural network
architectures widely used in computer vision. The size of the convolution
kernels determines both the expressiveness of convolutional neural networks
(CNN), as well as the number of learnable parameters. Increasing the network
capacity to capture rich pixel relationships requires increasing the number of
learnable parameters, often leading to overfitting and/or lack of robustness.
In this paper, we propose a powerful novel building block, the
hyper-convolution, which implicitly represents the convolution kernel as a
function of kernel coordinates. Hyper-convolutions enable decoupling the kernel
size, and hence its receptive field, from the number of learnable parameters.
In our experiments, focused on challenging biomedical image segmentation tasks,
we demonstrate that replacing regular convolutions with hyper-convolutions
leads to more efficient architectures that achieve improved accuracy. Our
analysis also shows that learned hyper-convolutions are naturally regularized,
which can offer better generalization performance. We believe that
hyper-convolutions can be a powerful building block in future neural network
architectures for computer vision tasks. We provide all of our code here:
https://github.com/tym002/Hyper-Convolution},
Year          = {2021},
Month         = {May},
Url           = {http://arxiv.org/abs/2105.10559v2},
File          = {2105.10559v2.pdf}
}
@article{1904.09737v3,
Author        = {Yongpei Zhu and Hongwei Fan and Kehong Yuan},
Title         = {Facial Expression Recognition Research Based on Deep Learning},
Eprint        = {1904.09737v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {With the development of deep learning, the structure of convolution neural
network is becoming more and more complex and the performance of object
recognition is getting better. However, the classification mechanism of
convolution neural networks is still an unsolved core problem. The main problem
is that convolution neural networks have too many parameters, which makes it
difficult to analyze them. In this paper, we design and train a convolution
neural network based on the expression recognition, and explore the
classification mechanism of the network. By using the Deconvolution
visualization method, the extremum point of the convolution neural network is
projected back to the pixel space of the original image, and we qualitatively
verify that the trained expression recognition convolution neural network forms
a detector for the specific facial action unit. At the same time, we design the
distance function to measure the distance between the presence of facial
feature unit and the maximal value of the response on the feature map of
convolution neural network. The greater the distance, the more sensitive the
feature map is to the facial feature unit. By comparing the maximum distance of
all facial feature elements in the feature graph, the mapping relationship
between facial feature element and convolution neural network feature map is
determined. Therefore, we have verified that the convolution neural network has
formed a detector for the facial Action unit in the training process to realize
the expression recognition.},
Year          = {2019},
Month         = {Apr},
Url           = {http://arxiv.org/abs/1904.09737v3},
File          = {1904.09737v3.pdf}
}
@article{2103.02096v2,
Author        = {Shiqing Fan and Liu Liying and Ye Luo},
Title         = {An Alternative Practice of Tropical Convolution to Traditional
  Convolutional Neural Networks},
Eprint        = {2103.02096v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Convolutional neural networks (CNNs) have been used in many machine learning
fields. In practical applications, the computational cost of convolutional
neural networks is often high with the deepening of the network and the growth
of data volume, mostly due to a large amount of multiplication operations of
floating-point numbers in convolution operations. To reduce the amount of
multiplications, we propose a new type of CNNs called Tropical Convolutional
Neural Networks (TCNNs) which are built on tropical convolutions in which the
multiplications and additions in conventional convolutional layers are replaced
by additions and min/max operations respectively. In addition, since tropical
convolution operators are essentially nonlinear operators, we expect TCNNs to
have higher nonlinear fitting ability than conventional CNNs. In the
experiments, we test and analyze several different architectures of TCNNs for
image classification tasks in comparison with similar-sized conventional CNNs.
The results show that TCNN can achieve higher expressive power than ordinary
convolutional layers on the MNIST and CIFAR10 image data set. In different
noise environments, there are wins and losses in the robustness of TCNN and
ordinary CNNs.},
Year          = {2021},
Month         = {Mar},
Url           = {http://arxiv.org/abs/2103.02096v2},
File          = {2103.02096v2.pdf}
}
@article{2005.05274v3,
Author        = {Dongsuk Kim and Geonhee Lee and Myungjae Lee and Shin Uk Kang and Dongmin Kim},
Title         = {Normalized Convolutional Neural Network},
Eprint        = {2005.05274v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {In this paper, we propose Normalized Convolutional Neural Network(NCNN). NCNN
is more fitted to a convolutional operator than other nomralizaiton methods.
The normalized process is similar to a normalization methods, but NCNN is more
adapative to sliced-inputs and corresponding the convolutional kernel. Therefor
NCNN can be targeted to micro-batch training. Normalizaing of NC is conducted
during convolutional process. In short, NC process is not usual normalization
and can not be realized in deep learning framework optimizing standard
convolution process. Hence we named this method 'Normalized Convolution'. As a
result, NC process has universal property which means NC can be applied to any
AI tasks involving convolution neural layer . Since NC don't need other
normalization layer, NCNN looks like convolutional version of Self Normalizing
Network.(SNN). Among micro-batch trainings, NCNN outperforms other
batch-independent normalization methods. NCNN archives these superiority by
standardizing rows of im2col matrix of inputs, which theoretically smooths the
gradient of loss. The code need to manipulate standard convolution neural
networks step by step. The code is available : https://github.com/kimdongsuk1/
NormalizedCNN.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.05274v3},
File          = {2005.05274v3.pdf}
}
@article{2005.14376v2,
Author        = {Rongfang Wang and Fan Ding and Licheng Jiao and Jia-Wei Chen and Bo Liu and Wenping Ma and Mi Wang},
Title         = {A Light-Weighted Convolutional Neural Network for Bitemporal SAR Image
  Change Detection},
Eprint        = {2005.14376v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.IV},
Abstract      = {Recently, many Convolution Neural Networks (CNN) have been successfully
employed in bitemporal SAR image change detection. However, most of the
existing networks are too heavy and occupy a large volume of memory for storage
and calculation. Motivated by this, in this paper, we propose a lightweight
neural network to reduce the computational and spatial complexity and
facilitate the change detection on an edge device. In the proposed network, we
replace normal convolutional layers with bottleneck layers that keep the same
number of channels between input and output. Next, we employ dilated
convolutional kernels with a few non-zero entries that reduce the running time
in convolutional operators. Comparing with the conventional convolutional
neural network, our light-weighted neural network will be more efficient with
fewer parameters. We verify our light-weighted neural network on four sets of
bitemporal SAR images. The experimental results show that the proposed network
can obtain better performance than the conventional CNN and has better model
generalization, especially on the challenging datasets with complex scenes.},
Year          = {2020},
Month         = {May},
Url           = {http://arxiv.org/abs/2005.14376v2},
File          = {2005.14376v2.pdf}
}
@article{1608.06049v2,
Author        = {Felix Juefei-Xu and Vishnu Naresh Boddeti and Marios Savvides},
Title         = {Local Binary Convolutional Neural Networks},
Eprint        = {1608.06049v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We propose local binary convolution (LBC), an efficient alternative to
convolutional layers in standard convolutional neural networks (CNN). The
design principles of LBC are motivated by local binary patterns (LBP). The LBC
layer comprises of a set of fixed sparse pre-defined binary convolutional
filters that are not updated during the training process, a non-linear
activation function and a set of learnable linear weights. The linear weights
combine the activated filter responses to approximate the corresponding
activated filter responses of a standard convolutional layer. The LBC layer
affords significant parameter savings, 9x to 169x in the number of learnable
parameters compared to a standard convolutional layer. Furthermore, the sparse
and binary nature of the weights also results in up to 9x to 169x savings in
model size compared to a standard convolutional layer. We demonstrate both
theoretically and experimentally that our local binary convolution layer is a
good approximation of a standard convolutional layer. Empirically, CNNs with
LBC layers, called local binary convolutional neural networks (LBCNN), achieves
performance parity with regular CNNs on a range of visual datasets (MNIST,
SVHN, CIFAR-10, and ImageNet) while enjoying significant computational savings.},
Year          = {2016},
Month         = {Aug},
Url           = {http://arxiv.org/abs/1608.06049v2},
File          = {1608.06049v2.pdf}
}
@article{1803.02129v1,
Author        = {Felix Altenberger and Claus Lenz},
Title         = {A Non-Technical Survey on Deep Convolutional Neural Network
  Architectures},
Eprint        = {1803.02129v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Artificial neural networks have recently shown great results in many
disciplines and a variety of applications, including natural language
understanding, speech processing, games and image data generation. One
particular application in which the strong performance of artificial neural
networks was demonstrated is the recognition of objects in images, where deep
convolutional neural networks are commonly applied. In this survey, we give a
comprehensive introduction to this topic (object recognition with deep
convolutional neural networks), with a strong focus on the evolution of network
architectures. Therefore, we aim to compress the most important concepts in
this field in a simple and non-technical manner to allow for future researchers
to have a quick general understanding.
  This work is structured as follows:
  1. We will explain the basic ideas of (convolutional) neural networks and
deep learning and examine their usage for three object recognition tasks: image
classification, object localization and object detection.
  2. We give a review on the evolution of deep convolutional neural networks by
providing an extensive overview of the most important network architectures
presented in chronological order of their appearances.},
Year          = {2018},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1803.02129v1},
File          = {1803.02129v1.pdf}
}
@article{1703.08245v1,
Author        = {Nicholas Cheney and Martin Schrimpf and Gabriel Kreiman},
Title         = {On the Robustness of Convolutional Neural Networks to Internal
  Architecture and Weight Perturbations},
Eprint        = {1703.08245v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Deep convolutional neural networks are generally regarded as robust function
approximators. So far, this intuition is based on perturbations to external
stimuli such as the images to be classified. Here we explore the robustness of
convolutional neural networks to perturbations to the internal weights and
architecture of the network itself. We show that convolutional networks are
surprisingly robust to a number of internal perturbations in the higher
convolutional layers but the bottom convolutional layers are much more fragile.
For instance, Alexnet shows less than a 30% decrease in classification
performance when randomly removing over 70% of weight connections in the top
convolutional or dense layers but performance is almost at chance with the same
perturbation in the first convolutional layer. Finally, we suggest further
investigations which could continue to inform the robustness of convolutional
networks to internal perturbations.},
Year          = {2017},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1703.08245v1},
File          = {1703.08245v1.pdf}
}
@article{2305.18070v1,
Author        = {Mart Keizer and Zeno Geradts and Meike Kombrink},
Title         = {Forensic Video Steganalysis in Spatial Domain by Noise Residual
  Convolutional Neural Network},
Eprint        = {2305.18070v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {This research evaluates a convolutional neural network (CNN) based approach
to forensic video steganalysis. A video steganography dataset is created to
train a CNN to conduct forensic steganalysis in the spatial domain. We use a
noise residual convolutional neural network to detect embedded secrets since a
steganographic embedding process will always result in the modification of
pixel values in video frames. Experimental results show that the CNN-based
approach can be an effective method for forensic video steganalysis and can
reach a detection rate of 99.96%. Keywords: Forensic, Steganalysis, Deep
Steganography, MSU StegoVideo, Convolutional Neural Networks},
Year          = {2023},
Month         = {May},
Url           = {http://arxiv.org/abs/2305.18070v1},
File          = {2305.18070v1.pdf}
}
@article{2010.01369v1,
Author        = {Eran Malach and Shai Shalev-Shwartz},
Title         = {Computational Separation Between Convolutional and Fully-Connected
  Networks},
Eprint        = {2010.01369v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Convolutional neural networks (CNN) exhibit unmatched performance in a
multitude of computer vision tasks. However, the advantage of using
convolutional networks over fully-connected networks is not understood from a
theoretical perspective. In this work, we show how convolutional networks can
leverage locality in the data, and thus achieve a computational advantage over
fully-connected networks. Specifically, we show a class of problems that can be
efficiently solved using convolutional networks trained with gradient-descent,
but at the same time is hard to learn using a polynomial-size fully-connected
network.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.01369v1},
File          = {2010.01369v1.pdf}
}
@article{1304.7948v2,
Author        = {Christian Osendorfer and Justin Bayer and Patrick van der Smagt},
Title         = {Convolutional Neural Networks learn compact local image descriptors},
Eprint        = {1304.7948v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {A standard deep convolutional neural network paired with a suitable loss
function learns compact local image descriptors that perform comparably to
state-of-the art approaches.},
Year          = {2013},
Month         = {Apr},
Url           = {http://arxiv.org/abs/1304.7948v2},
File          = {1304.7948v2.pdf}
}
@article{1809.00973v3,
Author        = {Philipp Petersen and Felix Voigtlaender},
Title         = {Equivalence of approximation by convolutional neural networks and
  fully-connected networks},
Eprint        = {1809.00973v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {math.FA},
Abstract      = {Convolutional neural networks are the most widely used type of neural
networks in applications. In mathematical analysis, however, mostly
fully-connected networks are studied. In this paper, we establish a connection
between both network architectures. Using this connection, we show that all
upper and lower bounds concerning approximation rates of {fully-connected}
neural networks for functions $f \in \mathcal{C}$ -- for an arbitrary function
class $\mathcal{C}$ -- translate to essentially the same bounds concerning
approximation rates of convolutional neural networks for functions $f \in
{\mathcal{C}^{equi}}$, with the class ${\mathcal{C}^{equi}}$ consisting of all
translation equivariant functions whose first coordinate belongs to
$\mathcal{C}$. All presented results consider exclusively the case of
convolutional neural networks without any pooling operation and with circular
convolutions, i.e., not based on zero-padding.},
Year          = {2018},
Month         = {Sep},
Note          = {Proc. Amer. Math. Soc. 148 (2020), 1567-1581},
Url           = {http://arxiv.org/abs/1809.00973v3},
File          = {1809.00973v3.pdf}
}
@article{2007.01556v1,
Author        = {Bin Wang and Bing Xue and Mengjie Zhang},
Title         = {Surrogate-assisted Particle Swarm Optimisation for Evolving
  Variable-length Transferable Blocks for Image Classification},
Eprint        = {2007.01556v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Deep convolutional neural networks have demonstrated promising performance on
image classification tasks, but the manual design process becomes more and more
complex due to the fast depth growth and the increasingly complex topologies of
convolutional neural networks. As a result, neural architecture search has
emerged to automatically design convolutional neural networks that outperform
handcrafted counterparts. However, the computational cost is immense, e.g.
22,400 GPU-days and 2,000 GPU-days for two outstanding neural architecture
search works named NAS and NASNet, respectively, which motivates this work. A
new effective and efficient surrogate-assisted particle swarm optimisation
algorithm is proposed to automatically evolve convolutional neural networks.
This is achieved by proposing a novel surrogate model, a new method of creating
a surrogate dataset and a new encoding strategy to encode variable-length
blocks of convolutional neural networks, all of which are integrated into a
particle swarm optimisation algorithm to form the proposed method. The proposed
method shows its effectiveness by achieving competitive error rates of 3.49% on
the CIFAR-10 dataset, 18.49% on the CIFAR-100 dataset, and 1.82% on the SVHN
dataset. The convolutional neural network blocks are efficiently learned by the
proposed method from CIFAR-10 within 3 GPU-days due to the acceleration
achieved by the surrogate model and the surrogate dataset to avoid the training
of 80.1% of convolutional neural network blocks represented by the particles.
Without any further search, the evolved blocks from CIFAR-10 can be
successfully transferred to CIFAR-100 and SVHN, which exhibits the
transferability of the block learned by the proposed method.},
Year          = {2020},
Month         = {Jul},
Url           = {http://arxiv.org/abs/2007.01556v1},
File          = {2007.01556v1.pdf}
}
@article{1905.02473v5,
Author        = {Gianluca Maguolo and Loris Nanni and Stefano Ghidoni},
Title         = {Ensemble of Convolutional Neural Networks Trained with Different
  Activation Functions},
Eprint        = {1905.02473v5},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Activation functions play a vital role in the training of Convolutional
Neural Networks. For this reason, to develop efficient and performing functions
is a crucial problem in the deep learning community. Key to these approaches is
to permit a reliable parameter learning, avoiding vanishing gradient problems.
The goal of this work is to propose an ensemble of Convolutional Neural
Networks trained using several different activation functions. Moreover, a
novel activation function is here proposed for the first time. Our aim is to
improve the performance of Convolutional Neural Networks in small/medium size
biomedical datasets. Our results clearly show that the proposed ensemble
outperforms Convolutional Neural Networks trained with standard ReLU as
activation function. The proposed ensemble outperforms with a p-value of 0.01
each tested stand-alone activation function; for reliable performance
comparison we have tested our approach in more than 10 datasets, using two
well-known Convolutional Neural Network: Vgg16 and ResNet50. MATLAB code used
here will be available at https://github.com/LorisNanni.},
Year          = {2019},
Month         = {May},
Url           = {http://arxiv.org/abs/1905.02473v5},
File          = {1905.02473v5.pdf}
}
@article{1406.2732v1,
Author        = {George Papandreou},
Title         = {Deep Epitomic Convolutional Neural Networks},
Eprint        = {1406.2732v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Deep convolutional neural networks have recently proven extremely competitive
in challenging image recognition tasks. This paper proposes the epitomic
convolution as a new building block for deep neural networks. An epitomic
convolution layer replaces a pair of consecutive convolution and max-pooling
layers found in standard deep convolutional neural networks. The main version
of the proposed model uses mini-epitomes in place of filters and computes
responses invariant to small translations by epitomic search instead of
max-pooling over image positions. The topographic version of the proposed model
uses large epitomes to learn filter maps organized in translational
topographies. We show that error back-propagation can successfully learn
multiple epitomic layers in a supervised fashion. The effectiveness of the
proposed method is assessed in image classification tasks on standard
benchmarks. Our experiments on Imagenet indicate improved recognition
performance compared to standard convolutional neural networks of similar
architecture. Our models pre-trained on Imagenet perform excellently on
Caltech-101. We also obtain competitive image classification results on the
small-image MNIST and CIFAR-10 datasets.},
Year          = {2014},
Month         = {Jun},
Url           = {http://arxiv.org/abs/1406.2732v1},
File          = {1406.2732v1.pdf}
}
@article{1602.07576v3,
Author        = {Taco S. Cohen and Max Welling},
Title         = {Group Equivariant Convolutional Networks},
Eprint        = {1602.07576v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a
natural generalization of convolutional neural networks that reduces sample
complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of
layer that enjoys a substantially higher degree of weight sharing than regular
convolution layers. G-convolutions increase the expressive capacity of the
network without increasing the number of parameters. Group convolution layers
are easy to use and can be implemented with negligible computational overhead
for discrete groups generated by translations, reflections and rotations.
G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
Year          = {2016},
Month         = {Feb},
Note          = {Proceedings of the International Conference on Machine Learning
  (ICML), 2016},
Url           = {http://arxiv.org/abs/1602.07576v3},
File          = {1602.07576v3.pdf}
}
@article{1712.06145v3,
Author        = {Dong-Qing Zhang},
Title         = {clcNet: Improving the Efficiency of Convolutional Neural Network using
  Channel Local Convolutions},
Eprint        = {1712.06145v3},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Depthwise convolution and grouped convolution has been successfully applied
to improve the efficiency of convolutional neural network (CNN). We suggest
that these models can be considered as special cases of a generalized
convolution operation, named channel local convolution(CLC), where an output
channel is computed using a subset of the input channels. This definition
entails computation dependency relations between input and output channels,
which can be represented by a channel dependency graph(CDG). By modifying the
CDG of grouped convolution, a new CLC kernel named interlaced grouped
convolution (IGC) is created. Stacking IGC and GC kernels results in a
convolution block (named CLC Block) for approximating regular convolution. By
resorting to the CDG as an analysis tool, we derive the rule for setting the
meta-parameters of IGC and GC and the framework for minimizing the
computational cost. A new CNN model named clcNet is then constructed using CLC
blocks, which shows significantly higher computational efficiency and fewer
parameters compared to state-of-the-art networks, when being tested using the
ImageNet-1K dataset. Source code is available at
https://github.com/dqzhang17/clcnet.torch .},
Year          = {2017},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1712.06145v3},
File          = {1712.06145v3.pdf}
}
@article{1801.00968v1,
Author        = {Yi Xiao and Xiang Cao and Xianyi Zhu and Renzhi Yang and Yan Zheng},
Title         = {Joint convolutional neural pyramid for depth map super-resolution},
Eprint        = {1801.00968v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {High-resolution depth map can be inferred from a low-resolution one with the
guidance of an additional high-resolution texture map of the same scene.
Recently, deep neural networks with large receptive fields are shown to benefit
applications such as image completion. Our insight is that super resolution is
similar to image completion, where only parts of the depth values are precisely
known. In this paper, we present a joint convolutional neural pyramid model
with large receptive fields for joint depth map super-resolution. Our model
consists of three sub-networks, two convolutional neural pyramids concatenated
by a normal convolutional neural network. The convolutional neural pyramids
extract information from large receptive fields of the depth map and guidance
map, while the convolutional neural network effectively transfers useful
structures of the guidance image to the depth image. Experimental results show
that our model outperforms existing state-of-the-art algorithms not only on
data pairs of RGB/depth images, but also on other data pairs like
color/saliency and color-scribbles/colorized images.},
Year          = {2018},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1801.00968v1},
File          = {1801.00968v1.pdf}
}
@article{1807.01418v2,
Author        = {Minho Ha and Younghoon Byeon and Youngjoo Lee and Sunggu Lee},
Title         = {Selective Deep Convolutional Neural Network for Low Cost Distorted Image
  Classification},
Eprint        = {1807.01418v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Deep convolutional neural networks have proven to be well suited for image
classification applications. However, if there is distortion in the image, the
classification accuracy can be significantly degraded, even with
state-of-the-art neural networks. The accuracy cannot be significantly improved
by simply training with distorted images. Instead, this paper proposes a
multiple neural network topology referred to as a selective deep convolutional
neural network. By modifying existing state-of-the-art neural networks in the
proposed manner, it is shown that a similar level of classification accuracy
can be achieved, but at a significantly lower cost. The cost reduction is
obtained primarily through the use of fewer weight parameters. Using fewer
weights reduces the number of multiply-accumulate operations and also reduces
the energy required for data accesses. Finally, it is shown that the
effectiveness of the proposed selective deep convolutional neural network can
be further improved by combining it with previously proposed network cost
reduction methods.},
Year          = {2018},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1807.01418v2},
File          = {1807.01418v2.pdf}
}
@article{2010.04257v1,
Author        = {Varsha Nair and Moitrayee Chatterjee and Neda Tavakoli and Akbar Siami Namin and Craig Snoeyink},
Title         = {Fast Fourier Transformation for Optimizing Convolutional Neural Networks
  in Object Recognition},
Eprint        = {2010.04257v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {This paper proposes to use Fast Fourier Transformation-based U-Net (a refined
fully convolutional networks) and perform image convolution in neural networks.
Leveraging the Fast Fourier Transformation, it reduces the image convolution
costs involved in the Convolutional Neural Networks (CNNs) and thus reduces the
overall computational costs. The proposed model identifies the object
information from the images. We apply the Fast Fourier transform algorithm on
an image data set to obtain more accessible information about the image data,
before segmenting them through the U-Net architecture. More specifically, we
implement the FFT-based convolutional neural network to improve the training
time of the network. The proposed approach was applied to publicly available
Broad Bioimage Benchmark Collection (BBBC) dataset. Our model demonstrated
improvement in training time during convolution from $600-700$ ms/step to
$400-500$ ms/step. We evaluated the accuracy of our model using Intersection
over Union (IoU) metric showing significant improvements.},
Year          = {2020},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2010.04257v1},
File          = {2010.04257v1.pdf}
}
@article{1705.07049v2,
Author        = {Hung Le and Ali Borji},
Title         = {What are the Receptive, Effective Receptive, and Projective Fields of
  Neurons in Convolutional Neural Networks?},
Eprint        = {1705.07049v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {In this work, we explain in detail how receptive fields, effective receptive
fields, and projective fields of neurons in different layers, convolution or
pooling, of a Convolutional Neural Network (CNN) are calculated. While our
focus here is on CNNs, the same operations, but in the reverse order, can be
used to calculate these quantities for deconvolutional neural networks. These
are important concepts, not only for better understanding and analyzing
convolutional and deconvolutional networks, but also for optimizing their
performance in real-world applications.},
Year          = {2017},
Month         = {May},
Url           = {http://arxiv.org/abs/1705.07049v2},
File          = {1705.07049v2.pdf}
}
@article{2211.16978v1,
Author        = {Jan Hohenheim and Mathias Fischler and Sara Zarubica and Jeremy Stucki},
Title         = {Combining Neuro-Evolution of Augmenting Topologies with Convolutional
  Neural Networks},
Eprint        = {2211.16978v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.NE},
Abstract      = {Current deep convolutional networks are fixed in their topology. We explore
the possibilites of making the convolutional topology a parameter itself by
combining NeuroEvolution of Augmenting Topologies (NEAT) with Convolutional
Neural Networks (CNNs) and propose such a system using blocks of Residual
Networks (ResNets). We then explain how our suggested system can only be built
once additional optimizations have been made, as genetic algorithms are way
more demanding than training per backpropagation. On the way there we explain
most of those buzzwords and offer a gentle and brief introduction to the most
important modern areas of machine learning},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2211.16978v1},
File          = {2211.16978v1.pdf}
}
@article{1909.03050v2,
Author        = {Kaisheng Liao and Yaodong Zhao and Jie Gu and Yaping Zhang and Yi Zhong},
Title         = {Sequential Convolutional Recurrent Neural Networks for Fast Automatic
  Modulation Classification},
Eprint        = {1909.03050v2},
DOI           = {10.1109/ACCESS.2021.3053427},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.SP},
Abstract      = {A novel and efficient end-to-end learning model for automatic modulation
classification is proposed for wireless spectrum monitoring applications, which
automatically learns from the time domain in-phase and quadrature data without
requiring the design of hand-crafted expert features. With the intuition of
convolutional layers with pooling serving as the role of front-end feature
distillation and dimensionality reduction, sequential convolutional recurrent
neural networks are developed to take complementary advantage of parallel
computing capability of convolutional neural networks and temporal sensitivity
of recurrent neural networks. Experimental results demonstrate that the
proposed architecture delivers overall superior performance in signal to noise
ratio range above -10~dB, and achieves significantly improved classification
accuracy from 80\% to 92.1\% at high signal to noise ratio range, while
drastically reduces the average training and prediction time by approximately
74% and 67%, respectively. Response patterns learned by the proposed
architecture are visualized to better understand the physics of the model.
Furthermore, a comparative study is performed to investigate the impacts of
various sequential convolutional recurrent neural network structure settings on
classification performance. A representative sequential convolutional recurrent
neural network architecture with the two-layer convolutional neural network and
subsequent two-layer long short-term memory neural network is developed to
suggest the option for fast automatic modulation classification.},
Year          = {2019},
Month         = {Sep},
Url           = {http://arxiv.org/abs/1909.03050v2},
File          = {1909.03050v2.pdf}
}
@article{1701.04489v1,
Author        = {Tapabrata Ghosh},
Title         = {Towards a New Interpretation of Separable Convolutions},
Eprint        = {1701.04489v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {In recent times, the use of separable convolutions in deep convolutional
neural network architectures has been explored. Several researchers, most
notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in
their deep architectures and have demonstrated state of the art or close to
state of the art performance. However, the underlying mechanism of action of
separable convolutions are still not fully understood. Although their
mathematical definition is well understood as a depthwise convolution followed
by a pointwise convolution, deeper interpretations such as the extreme
Inception hypothesis (Chollet, 2016) have failed to provide a thorough
explanation of their efficacy. In this paper, we propose a hybrid
interpretation that we believe is a better model for explaining the efficacy of
separable convolutions.},
Year          = {2017},
Month         = {Jan},
Url           = {http://arxiv.org/abs/1701.04489v1},
File          = {1701.04489v1.pdf}
}
@article{1907.03100v1,
Author        = {Reinhard Heckel},
Title         = {Regularizing linear inverse problems with convolutional neural networks},
Eprint        = {1907.03100v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Deep convolutional neural networks trained on large datsets have emerged as
an intriguing alternative for compressing images and solving inverse problems
such as denoising and compressive sensing. However, it has only recently been
realized that even without training, convolutional networks can function as
concise image models, and thus regularize inverse problems. In this paper, we
provide further evidence for this finding by studying variations of
convolutional neural networks that map few weight parameters to an image. The
networks we consider only consist of convolutional operations, with either
fixed or parameterized filters followed by ReLU non-linearities. We demonstrate
that with both fixed and parameterized convolutional filters those networks
enable representing images with few coefficients. What is more, the
underparameterization enables regularization of inverse problems, in particular
recovering an image from few observations. We show that, similar to standard
compressive sensing guarantees, on the order of the number of model parameters
many measurements suffice for recovering an image from compressive
measurements. Finally, we demonstrate that signal recovery with a un-trained
convolutional network outperforms standard l1 and total variation minimization
for magnetic resonance imaging (MRI).},
Year          = {2019},
Month         = {Jul},
Url           = {http://arxiv.org/abs/1907.03100v1},
File          = {1907.03100v1.pdf}
}
@article{1603.06759v1,
Author        = {Yanwei Pang and Manli Sun and Xiaoheng Jiang and Xuelong Li},
Title         = {Convolution in Convolution for Network in Network},
Eprint        = {1603.06759v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Network in Netwrok (NiN) is an effective instance and an important extension
of Convolutional Neural Network (CNN) consisting of alternating convolutional
layers and pooling layers. Instead of using a linear filter for convolution,
NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to
replace the linear filter. Because of the powerfulness of MLP and $ 1\times 1 $
convolutions in spatial domain, NiN has stronger ability of feature
representation and hence results in better recognition rate. However, MLP
itself consists of fully connected layers which give rise to a large number of
parameters. In this paper, we propose to replace dense shallow MLP with sparse
shallow MLP. One or more layers of the sparse shallow MLP are sparely connected
in the channel dimension or channel-spatial domain. The proposed method is
implemented by applying unshared convolution across the channel dimension and
applying shared convolution across the spatial dimension in some computational
layers. The proposed method is called CiC. Experimental results on the CIFAR10
dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the
effectiveness of the proposed CiC method.},
Year          = {2016},
Month         = {Mar},
Url           = {http://arxiv.org/abs/1603.06759v1},
File          = {1603.06759v1.pdf}
}
@article{2202.06673v1,
Author        = {Zhongxia Zhang and Mingwen Wang},
Title         = {Convolutional Neural Network with Convolutional Block Attention Module
  for Finger Vein Recognition},
Eprint        = {2202.06673v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {Convolutional neural networks have become a popular research in the field of
finger vein recognition because of their powerful image feature representation.
However, most researchers focus on improving the performance of the network by
increasing the CNN depth and width, which often requires high computational
effort. Moreover, we can notice that not only the importance of pixels in
different channels is different, but also the importance of pixels in different
positions of the same channel is different. To reduce the computational effort
and to take into account the different importance of pixels, we propose a
lightweight convolutional neural network with a convolutional block attention
module (CBAM) for finger vein recognition, which can achieve a more accurate
capture of visual structures through an attention mechanism. First, image
sequences are fed into a lightweight convolutional neural network we designed
to improve visual features. Afterwards, it learns to assign feature weights in
an adaptive manner with the help of a convolutional block attention module. The
experiments are carried out on two publicly available databases and the results
demonstrate that the proposed method achieves a stable, highly accurate, and
robust performance in multimodal finger recognition.},
Year          = {2022},
Month         = {Feb},
Url           = {http://arxiv.org/abs/2202.06673v1},
File          = {2202.06673v1.pdf}
}
@article{1805.07857v2,
Author        = {Stefan C. Schonsheck and Bin Dong and Rongjie Lai},
Title         = {Parallel Transport Convolution: A New Tool for Convolutional Neural
  Networks on Manifolds},
Eprint        = {1805.07857v2},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.LG},
Abstract      = {Convolution has been playing a prominent role in various applications in
science and engineering for many years. It is the most important operation in
convolutional neural networks. There has been a recent growth of interests of
research in generalizing convolutions on curved domains such as manifolds and
graphs. However, existing approaches cannot preserve all the desirable
properties of Euclidean convolutions, namely compactly supported filters,
directionality, transferability across different manifolds. In this paper we
develop a new generalization of the convolution operation, referred to as
parallel transport convolution (PTC), on Riemannian manifolds and their
discrete counterparts. PTC is designed based on the parallel transportation
which is able to translate information along a manifold and to intrinsically
preserve directionality. PTC allows for the construction of compactly supported
filters and is also robust to manifold deformations. This enables us to preform
wavelet-like operations and to define deep convolutional neural networks on
curved domains.},
Year          = {2018},
Month         = {May},
Url           = {http://arxiv.org/abs/1805.07857v2},
File          = {1805.07857v2.pdf}
}
@article{2111.04961v1,
Author        = {Nathan Leroux and Arnaud De Riz and Dédalo Sanz-Hernández and Danijela Marković and Alice Mizrahi and Julie Grollier},
Title         = {Convolutional Neural Networks with Radio-Frequency Spintronic
  Nano-Devices},
Eprint        = {2111.04961v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.ET},
Abstract      = {Convolutional neural networks are state-of-the-art and ubiquitous in modern
signal processing and machine vision. Nowadays, hardware solutions based on
emerging nanodevices are designed to reduce the power consumption of these
networks. Spintronics devices are promising for information processing because
of the various neural and synaptic functionalities they offer. However, due to
their low OFF/ON ratio, performing all the multiplications required for
convolutions in a single step with a crossbar array of spintronic memories
would cause sneak-path currents. Here we present an architecture where synaptic
communications have a frequency selectivity that prevents crosstalk caused by
sneak-path currents. We first demonstrate how a chain of spintronic resonators
can function as synapses and make convolutions by sequentially rectifying
radio-frequency signals encoding consecutive sets of inputs. We show that a
parallel implementation is possible with multiple chains of spintronic
resonators to avoid storing intermediate computational steps in memory. We
propose two different spatial arrangements for these chains. For each of them,
we explain how to tune many artificial synapses simultaneously, exploiting the
synaptic weight sharing specific to convolutions. We show how information can
be transmitted between convolutional layers by using spintronic oscillators as
artificial microwave neurons. Finally, we simulate a network of these
radio-frequency resonators and spintronic oscillators to solve the MNIST
handwritten digits dataset, and obtain results comparable to software
convolutional neural networks. Since it can run convolutional neural networks
fully in parallel in a single step with nano devices, the architecture proposed
in this paper is promising for embedded applications requiring machine vision,
such as autonomous driving.},
Year          = {2021},
Month         = {Nov},
Url           = {http://arxiv.org/abs/2111.04961v1},
File          = {2111.04961v1.pdf}
}
@article{1901.07375v1,
Author        = {Jay Hoon Jung and Yousun Shin and YoungMin Kwon},
Title         = {Extension of Convolutional Neural Network with General Image Processing
  Kernels},
Eprint        = {1901.07375v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {We applied pre-defined kernels also known as filters or masks developed for
image processing to convolution neural network. Instead of letting neural
networks find its own kernels, we used 41 different general-purpose kernels of
blurring, edge detecting, sharpening, discrete cosine transformation, etc. for
the first layer of the convolution neural networks. This architecture, thus
named as general filter convolutional neural network (GFNN), can reduce
training time by 30% with a better accuracy compared to the regular
convolutional neural network (CNN). GFNN also can be trained to achieve 90%
accuracy with only 500 samples. Furthermore, even though these kernels are not
specialized for the MNIST dataset, we achieved 99.56% accuracy without ensemble
nor any other special algorithms.},
Year          = {2019},
Month         = {Jan},
Note          = {TENCON 2018},
Url           = {http://arxiv.org/abs/1901.07375v1},
File          = {1901.07375v1.pdf}
}
@article{2002.07754v1,
Author        = {Elena Limonova and Alexander Sheshkus and Dmitry Nikolaev},
Title         = {Computational optimization of convolutional neural networks using
  separated filters architecture},
Eprint        = {2002.07754v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {This paper considers a convolutional neural network transformation that
reduces computation complexity and thus speedups neural network processing.
Usage of convolutional neural networks (CNN) is the standard approach to image
recognition despite the fact they can be too computationally demanding, for
example for recognition on mobile platforms or in embedded systems. In this
paper we propose CNN structure transformation which expresses 2D convolution
filters as a linear combination of separable filters. It allows to obtain
separated convolutional filters by standard training algorithms. We study the
computation efficiency of this structure transformation and suggest fast
implementation easily handled by CPU or GPU. We demonstrate that CNNs designed
for letter and digit recognition of proposed structure show 15% speedup without
accuracy loss in industrial image recognition system. In conclusion, we discuss
the question of possible accuracy decrease and the application of proposed
transformation to different recognition problems. convolutional neural
networks, computational optimization, separable filters, complexity reduction.},
Year          = {2020},
Month         = {Feb},
Note          = {International Journal of Applied Engineering Research (ISSN
  0973-4562), Volume 11, Number 11 (2016), pp 7491-7494},
Url           = {http://arxiv.org/abs/2002.07754v1},
File          = {2002.07754v1.pdf}
}
@article{2210.14184v1,
Author        = {Tian-Yi Zhou and Xiaoming Huo},
Title         = {Learning Ability of Interpolating Convolutional Neural Networks},
Eprint        = {2210.14184v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {stat.ML},
Abstract      = {It is frequently observed that overparameterized neural networks generalize
well. Regarding such phenomena, existing theoretical work mainly devotes to
linear settings or fully connected neural networks. This paper studies learning
ability of an important family of deep neural networks, deep convolutional
neural networks (DCNNs), under underparameterized and overparameterized
settings. We establish the best learning rates of underparameterized DCNNs
without parameter restrictions presented in the literature. We also show that,
by adding well defined layers to an underparameterized DCNN, we can obtain some
interpolating DCNNs that maintain the good learning rates of the
underparameterized DCNN. This result is achieved by a novel network deepening
scheme designed for DCNNs. Our work provides theoretical verification on how
overfitted DCNNs generalize well.},
Year          = {2022},
Month         = {Oct},
Url           = {http://arxiv.org/abs/2210.14184v1},
File          = {2210.14184v1.pdf}
}
@article{1712.00720v1,
Author        = {Huichao Hong and Lixin Zheng and Jianqing Zhu and Shuwan Pan and Kaiting Zhou},
Title         = {Automatic Recognition of Coal and Gangue based on Convolution Neural
  Network},
Eprint        = {1712.00720v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {cs.CV},
Abstract      = {We designed a gangue sorting system,and built a convolutional neural network
model based on AlexNet. Data enhancement and transfer learning are used to
solve the problem which the convolution neural network has insufficient
training data in the training stage. An object detection and region clipping
algorithm is proposed to adjust the training image data to the optimum size.
Compared with traditional neural network and SVM algorithm, this algorithm has
higher recognition rate for coal and coal gangue, and provides important
reference for identification and separation of coal and gangue.},
Year          = {2017},
Month         = {Dec},
Url           = {http://arxiv.org/abs/1712.00720v1},
File          = {1712.00720v1.pdf}
}
@article{1905.12281v1,
Author        = {Diego Valsesia and Giulia Fracastoro and Enrico Magli},
Title         = {Image Denoising with Graph-Convolutional Neural Networks},
Eprint        = {1905.12281v1},
ArchivePrefix = {arXiv},
PrimaryClass  = {eess.IV},
Abstract      = {Recovering an image from a noisy observation is a key problem in signal
processing. Recently, it has been shown that data-driven approaches employing
convolutional neural networks can outperform classical model-based techniques,
because they can capture more powerful and discriminative features. However,
since these methods are based on convolutional operations, they are only
capable of exploiting local similarities without taking into account non-local
self-similarities. In this paper we propose a convolutional neural network that
employs graph-convolutional layers in order to exploit both local and non-local
similarities. The graph-convolutional layers dynamically construct
neighborhoods in the feature space to detect latent correlations in the feature
maps produced by the hidden layers. The experimental results show that the
proposed architecture outperforms classical convolutional neural networks for
the denoising task.},
Year          = {2019},
Month         = {May},
Url           = {http://arxiv.org/abs/1905.12281v1},
File          = {1905.12281v1.pdf}
}
